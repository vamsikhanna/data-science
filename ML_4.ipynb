{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54be8a91",
   "metadata": {},
   "source": [
    "# 1. What are the key tasks involved in getting ready to work with machine learning modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac3f0d",
   "metadata": {},
   "source": [
    "The key tasks involved in getting ready to work with machine learning modeling include:\n",
    "\n",
    "Defining the problem: The first step is to clearly define the problem that you want to solve using machine learning. This includes understanding the problem domain, the type of data that is available, and the performance metrics that will be used to evaluate the solution.\n",
    "\n",
    "Collecting and preparing the data: The next step is to collect and prepare the data that will be used to train and evaluate the machine learning model. This includes cleaning and preprocessing the data, as well as exploring and visualizing the data to gain insights into its structure and characteristics.\n",
    "\n",
    "Selecting a model and a training method: After exploring and preparing the data, the next step is to select a machine learning model and a training method. This includes choosing the appropriate algorithm for the problem, and selecting the appropriate hyperparameters for the model.\n",
    "\n",
    "Splitting the data: It's important to split the data into training, validation and testing sets, this allows for a fair evaluation of the model and prevents overfitting.\n",
    "\n",
    "Training the model: Once the model and training method are selected, the model is trained on the prepared data. The model learns the patterns and relationships in the data, and the parameters of the model are adjusted to minimize the error on the training data.\n",
    "\n",
    "Evaluating the model: After the model is trained, it is evaluated on a hold-out dataset or using cross-validation techniques to assess its performance. This includes measuring the model's accuracy, precision, and recall.\n",
    "\n",
    "Fine-tuning and Deploying the model: Once the model has been trained and evaluated, it may need to be fine-tuned to improve its performance. The final model is then deployed in a production environment, where it can be used to make predictions or decisions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef99aea",
   "metadata": {},
   "source": [
    "# 2. What are the different forms of data used in machine learning? Give a specific example for each of them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c24295",
   "metadata": {},
   "source": [
    "There are several forms of data used in machine learning, including:\n",
    "\n",
    "Numerical data: Numerical data is data that can be represented as numbers. Examples include continuous variables such as temperature, weight, and height, and discrete variables such as counts and rankings. An example of numerical data is a dataset of housing prices, where the input variables are the number of bedrooms, square footage, and location, and the output variable is the price of the house.\n",
    "\n",
    "Categorical data: Categorical data is data that can be divided into categories or groups. Examples include nominal variables such as gender, color, and brand, and ordinal variables such as levels of education and customer satisfaction ratings. An example of categorical data is a dataset of iris flowers, where the input variables are the sepal length, sepal width, petal length, and petal width, and the output variable is the species of the iris flower.\n",
    "\n",
    "Text data: Text data is data that is represented as words or sentences. Examples include text documents, emails, and social media posts. An example of text data is a dataset of customer reviews of a product, where the input variables are the customer reviews, and the output variable is the sentiment of the review (positive, negative, or neutral).\n",
    "\n",
    "Image data: Image data is data that is represented as pictures or images. Examples include photographs, videos, and medical images. An example of image data is a dataset of handwritten digits, where the input variables are images of handwritten digits, and the output variable is the digit itself (0-9).\n",
    "\n",
    "Time series data: Time series data is data that is collected over time. Examples include financial data, weather data, and sensor data. An example of time series data is a dataset of daily stock prices, where the input variables are historical stock prices and other market indicators, and the output variable is the predicted stock price for the next day.\n",
    "\n",
    "Audio data: Audio data is data that is represented as sound or speech. Examples include speech recordings, music, and sound effects. An example of audio data is a dataset of speech samples, where the input variables are audio recordings of speech, and the output variable is the transcription of the speech.\n",
    "\n",
    "These are some of the main forms of data used in machine learning, depending on the problem and data availability, the most appropriate form of data would be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15475d2",
   "metadata": {},
   "source": [
    "# 3. Distinguish:\n",
    "\n",
    "           1. Numeric vs. categorical attributes\n",
    "\n",
    "            2. Feature selection vs. dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13c17a8",
   "metadata": {},
   "source": [
    "Numeric vs. categorical attributes:\n",
    "Numeric attributes are variables that can be represented as numbers and can be either continuous or discrete. They can be used in mathematical calculations, and can be sorted and ordered. Examples include temperature, weight, and height. Categorical attributes are variables that can be divided into categories or groups. They are not numerical and cannot be used in mathematical calculations, but can be used to group and classify data. Examples include gender, color, and brand.\n",
    "\n",
    "Feature selection vs. dimensionality reduction:\n",
    "Feature selection is the process of selecting a subset of the most relevant features from a dataset to use in a machine learning model. The goal is to select the features that have the most impact on the outcome and remove the ones that are irrelevant or redundant. Dimensionality reduction, on the other hand, is the process of reducing the number of features in a dataset by projecting the data onto a lower-dimensional subspace. The goal is to keep the most important information while removing less important information.\n",
    "\n",
    "In feature selection, we focus on choosing the most informative features from the dataset, while in dimensionality reduction, the goal is to transform the data in such a way that the features that are not important for the problem at hand are removed or combined, resulting in a more compact representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0849563",
   "metadata": {},
   "source": [
    "# 4. Make quick notes on any two of the following:\n",
    "\n",
    "            1. The histogram\n",
    "\n",
    "             2. Use a scatter plot\n",
    "\n",
    "              3.PCA (Personal Computer Aid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c373db9",
   "metadata": {},
   "source": [
    "The histogram:\n",
    "A histogram is a graphical representation of the distribution of a dataset. It is an estimate of the probability distribution of a continuous variable. It shows the frequency of each value in the dataset. The x-axis represents the different bins (or ranges of values) and the y-axis represents the frequency count of the values that fall within each bin. Histograms can be used to visualize the distribution of the data and identify outliers, skewness, and other patterns in the data.\n",
    "\n",
    "Scatter Plot:\n",
    "A scatter plot is a graphical representation of the relationship between two or more variables. It is used to visualize the relationship between two numerical variables. The x-axis represents one variable and the y-axis represents the other variable. Each point on the scatter plot represents a single observation from the dataset. Scatter plots can be used to visualize the relationship between two variables and identify patterns and outliers in the data. Scatter plots can also be used to explore the relationship between more than two variables by using different colors, symbols or shapes to represent different groups.\n",
    "\n",
    "PCA (Principal Component Analysis):\n",
    "PCA is a technique for dimensionality reduction that is used to find the most important features (or principal components) of a dataset. The goal is to reduce the number of features in the dataset by projecting the data onto a lower-dimensional subspace. PCA achieves this by finding the directions in the data that have the highest variance and retaining those directions as the principal components. By doing this, PCA is able to capture most of the information in the data while reducing the dimensionality of the dataset. PCA is a powerful technique for data visualization, data compression, and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebcbd20",
   "metadata": {},
   "source": [
    "# 5. Why is it necessary to investigate data? Is there a discrepancy in how qualitative and quantitative data are explored?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd641b1",
   "metadata": {},
   "source": [
    "Investigating data is necessary to gain insights into the characteristics and patterns of the data, and to identify any issues or problems that may need to be addressed before building a machine learning model. There are several reasons why it is important to investigate data, including:\n",
    "\n",
    "Identifying missing or incorrect data: Investigating data can help identify any missing or incorrect data that may need to be cleaned or corrected before building a model.\n",
    "\n",
    "Understanding the distribution of the data: Investigating data can help understand the distribution of the data, including the range, mean, median, and standard deviation, which can inform the choice of machine learning algorithm and the feature scaling.\n",
    "\n",
    "Identifying outliers and anomalies: Investigating data can help identify outliers and anomalies that may need to be removed or handled differently before building a model.\n",
    "\n",
    "Identifying patterns and relationships: Investigating data can help identify patterns and relationships in the data that can be used to build a more effective machine learning model.\n",
    "\n",
    "Identifying potential biases: Investigating data can help identify potential biases in the data that may impact the results of the machine learning model.\n",
    "\n",
    "The methods used to investigate data will depend on the type of data, whether it is qualitative or quantitative. Quantitative data is data that can be represented as numbers, such as numerical data and can be analyzed using statistical methods, while qualitative data is data that cannot be represented as numbers, such as text data, image data and audio data. Qualitative data is usually analyzed through descriptive statistics and visualizations, while quantitative data is analyzed through inferential statistics and visualizations.\n",
    "\n",
    "In summary, investigating data is a crucial step in the machine learning process and it is necessary to understand the data and identify any issues or problems that may need to be addressed before building a model. The methods used to investigate data will depend on the type of data, whether it is quantitative or qualitative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dffeb6",
   "metadata": {},
   "source": [
    "# 6. What are the various histogram shapes? What exactly are ‘bins'?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5357421",
   "metadata": {},
   "source": [
    "A histogram is a graphical representation of the distribution of a dataset. It is used to visualize the frequency of each value in the dataset. The x-axis represents the different bins (or ranges of values) and the y-axis represents the frequency count of the values that fall within each bin.\n",
    "\n",
    "There are several histogram shapes that can occur in data, including:\n",
    "\n",
    "Normal Distribution: A histogram that has a bell-shaped curve is said to be normally distributed. A normal distribution indicates that the data is symmetric and the mean, median, and mode are equal.\n",
    "\n",
    "Skewed Distribution: A histogram that is not symmetric is said to be skewed. A skewed distribution indicates that the data is not symmetric, and the mean, median, and mode are not equal. A distribution can be skewed to the left or to the right. A left-skewed distribution has a tail on the left side, and the mode is on the right side. A right-skewed distribution has a tail on the right side and the mode is on the left side.\n",
    "\n",
    "Multi-modal Distribution: A histogram that has more than one peak is said to be multi-modal. A multi-modal distribution indicates that the data has more than one underlying distribution.\n",
    "\n",
    "Unimodal Distribution: A histogram that has one peak is said to be unimodal. A unimodal distribution indicates that the data has one underlying distribution.\n",
    "\n",
    "Regarding 'bins', bins are the intervals or ranges of values on the x-axis of a histogram. The width of the bin can be adjusted to better show the distribution of the data. A histogram can be created with a small number of bins, which results in a coarser representation of the data, or a large number of bins, which results in a finer representation of the data. The choice of bin size depends on the data, and it should be chosen to best show the distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd422dc",
   "metadata": {},
   "source": [
    "# 7. How do we deal with data outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c38655b",
   "metadata": {},
   "source": [
    "Outliers are values that are significantly different from the other values in the dataset. They can have a significant impact on the results of a machine learning model and can skew the model's performance if they are not handled properly. There are several ways to deal with data outliers, including:\n",
    "\n",
    "Removing outliers: One of the simplest ways to deal with outliers is to remove them from the dataset. This can be done by setting a threshold on the data and removing any values that fall outside of that threshold.\n",
    "\n",
    "Transforming the data: Another way to deal with outliers is to transform the data using a mathematical function. This can include techniques such as log transformation, square root transformation, and Box-Cox transformation which can help to reduce the influence of outliers.\n",
    "\n",
    "Imputing missing values: If the outlier is caused by a missing value, then it can be imputed using techniques like mean, median or mode imputation.\n",
    "\n",
    "Binning: Binning is the process of grouping a set of continuous numerical data into a set of discrete \"bins\" for histogram representation. Binning can be useful in cases where outliers are caused by measurement errors.\n",
    "\n",
    "Anomaly Detection: Anomaly detection is a technique used to identify data points that deviate from the expected pattern of the data. This can be used to identify and remove outliers.\n",
    "\n",
    "Treating the outliers: Instead of removing the outliers, they can be treated as a separate category which can be useful in certain cases, for example, in fraud detection, an outlier can be a fraudulent transaction.\n",
    "\n",
    "It is important to note that while removing outliers is an easy solution, it can also remove valuable information from the dataset. Therefore, it's important to examine the outliers before removing them and consider the context of the problem before deciding which method to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c711d835",
   "metadata": {},
   "source": [
    "# 8. What are the various central inclination measures? Why does mean vary too much from median in certain data sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbc264d",
   "metadata": {},
   "source": [
    "Central inclination measures are statistical measures that are used to describe the central tendency or average value of a dataset. The most common central inclination measures include:\n",
    "\n",
    "Mean: The mean is the arithmetic average of a dataset. It is calculated by summing all the values in the dataset and dividing by the number of values. The mean is sensitive to outliers and skewed data.\n",
    "\n",
    "Median: The median is the middle value of a dataset when it is ordered. It is not affected by outliers or skewed data and it is a robust measure of central tendency.\n",
    "\n",
    "Mode: The mode is the most frequent value in a dataset. It is useful when dealing with categorical data.\n",
    "\n",
    "Geometric Mean: The geometric mean is a measure of central tendency that is calculated by multiplying all the values in the dataset together and taking the nth root, where n is the number of values in the dataset.\n",
    "\n",
    "Harmonic Mean: The harmonic mean is another measure of central tendency that is calculated by taking the reciprocal of the mean of the reciprocals of the values in the dataset.\n",
    "\n",
    "Mean can vary too much from median in certain datasets when the dataset has outliers or skewness. In a dataset with outliers, the mean is pulled in the direction of the outliers while the median is not affected by them, resulting in a large difference between the mean and median. Similarly, in a skewed dataset, the mean is pulled in the direction of the skewness while the median is not affected by it, resulting in a large difference between the mean and median.\n",
    "\n",
    "In these cases, the median is a more robust measure of central tendency than the mean, and it is less sensitive to outliers and skewness in the data. In general, it is important to consider both the mean and the median when analyzing data to get a better understanding of the central tendency of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a7885e",
   "metadata": {},
   "source": [
    "# 9. Describe how a scatter plot can be used to investigate bivariate relationships. Is it possible to find outliers using a scatter plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a84e109",
   "metadata": {},
   "source": [
    "A scatter plot is a graphical representation of the relationship between two variables. It is used to visualize the relationship between two numerical variables. The x-axis represents one variable and the y-axis represents the other variable. Each point on the scatter plot represents a single observation from the dataset.\n",
    "\n",
    "Scatter plots can be used to investigate bivariate relationships by showing the relationship between two variables. By looking at the scatter plot, we can identify patterns and trends in the data such as linear, non-linear, positive, negative or no correlation. Scatter plots can also be used to explore the relationship between more than two variables by using different colors, symbols or shapes to represent different groups.\n",
    "\n",
    "It is possible to find outliers using a scatter plot by looking for data points that are far away from the other points. Outliers are values that are significantly different from the other values in the dataset and can be easily identified as points that fall outside of the cluster of points on the scatter plot.\n",
    "\n",
    "Additionally, scatter plots can also be used to investigate the distribution of the data and identify any outliers, skewness, and other patterns in the data. By visualizing the data in this way, we can gain a better understanding of the underlying relationships in the data and identify any issues or problems that may need to be addressed before building a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dfb2fe",
   "metadata": {},
   "source": [
    "# 10. Describe how cross-tabs can be used to figure out how two variables are related.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d5172",
   "metadata": {},
   "source": [
    "Cross-tabs, also known as contingency tables or crosstabs, are a way of summarizing the relationship between two categorical variables. They provide a quick and easy way to see how the values of one variable are distributed across the levels of another variable.\n",
    "\n",
    "To create a cross-tab, the values of one variable are placed along the rows of the table, and the values of the other variable are placed along the columns. The cells of the table then show the count or percentage of observations that fall into each combination of the two variables.\n",
    "\n",
    "For example, if we want to figure out how gender and purchasing behavior are related, we can create a cross-tab with gender along the rows and purchasing behavior along the columns. The cells of the table would show the count or percentage of observations for each combination of gender and purchasing behavior (e.g. male and purchased, female and not purchased, etc.).\n",
    "\n",
    "Cross-tabs can be used to figure out how two variables are related by looking at the distribution of values in the cells of the table. If the values of one variable are evenly distributed across the levels of the other variable, we can conclude that the two variables are not related. However, if the values of one variable are concentrated in certain levels of the other variable, we can conclude that the two variables are related.\n",
    "\n",
    "Additionally, cross-tabs can also be used to identify any patterns or trends in the data, such as which combinations of the two variables are most common or least common, or if there are any significant differences between the levels of one variable.\n",
    "\n",
    "It's worth noting that cross-tabs can be useful when the two variables are categorical, and it can provide a quick and easy way to see how the values of one variable are distributed across the levels of another variable.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
