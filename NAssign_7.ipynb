{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab4ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Explain the architecture of BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c492ae4",
   "metadata": {},
   "source": [
    "BERT, which stands for \"Bidirectional Encoder Representations from Transformers,\" is a powerful natural language processing (NLP) model introduced by Google AI in 2018. It revolutionized the field of NLP by achieving state-of-the-art results on a wide range of NLP tasks, including text classification, named entity recognition, question answering, and more. The architecture of BERT is based on the transformer architecture, which was introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017.\n",
    "\n",
    "The key features of the BERT architecture are:\n",
    "\n",
    "1. **Transformer-Based Architecture:** BERT is built upon the transformer architecture, which is known for its effectiveness in capturing long-range dependencies in sequences. Transformers use self-attention mechanisms to process input data in parallel, making them highly parallelizable and efficient for training on large datasets.\n",
    "\n",
    "2. **Bidirectional Context:** Unlike previous models that read text in a unidirectional manner (e.g., from left to right), BERT is bidirectional. It considers context from both the left and right sides of each word in a sentence. This bidirectional context understanding is achieved through the use of a \"masked language model\" pretraining task.\n",
    "\n",
    "3. **Pretraining:** BERT is pretrained on a massive corpus of text data using two unsupervised pretraining tasks:\n",
    "   - **Masked Language Model (MLM):** In this task, a random subset of words in a sentence is replaced with [MASK] tokens, and the model is trained to predict the original words. This encourages the model to understand context and relationships between words.\n",
    "   - **Next Sentence Prediction (NSP):** BERT is trained to predict whether a sentence follows another sentence in a document. This task helps the model capture document-level context.\n",
    "\n",
    "4. **Architecture Details:**\n",
    "   - **Layers:** BERT consists of multiple transformer encoder layers stacked on top of each other. The original BERT model, known as BERT Base, has 12 layers, while BERT Large has 24 layers.\n",
    "   - **Attention Heads:** Each transformer layer in BERT has multiple attention heads, which allows the model to focus on different parts of the input text.\n",
    "   - **Embeddings:** BERT uses learned embeddings for input tokens, segment embeddings to distinguish between different sentences in a document, and position embeddings to encode the order of tokens.\n",
    "\n",
    "5. **Fine-Tuning:** After pretraining, BERT can be fine-tuned on downstream NLP tasks with labeled data. Fine-tuning involves adding task-specific layers on top of the pretrained BERT model and training the entire model on the specific task.\n",
    "\n",
    "6. **Contextualized Representations:** BERT produces contextualized word embeddings, meaning that the representation of a word depends on its context in the sentence. This enables BERT to capture nuances and polysemy in language.\n",
    "\n",
    "7. **Multilingual Variants:** BERT has multilingual variants trained on text from multiple languages, making it capable of understanding and generating text in a wide range of languages.\n",
    "\n",
    "BERT's architecture and pretraining tasks have become the foundation for many subsequent NLP models and have significantly advanced the state of the art in various NLP applications. BERT's ability to capture bidirectional context and its contextualized embeddings have made it a fundamental tool for a wide range of NLP tasks, including sentiment analysis, text summarization, machine translation, and question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Explain Masked Language Modeling (MLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e250ce40",
   "metadata": {},
   "source": [
    "Masked Language Modeling (MLM) is a type of unsupervised pretraining task used in natural language processing (NLP) and specifically in transformer-based models like BERT (Bidirectional Encoder Representations from Transformers). MLM is designed to train models to understand contextual relationships between words in a sentence by predicting masked or hidden words.\n",
    "\n",
    "Here's how the Masked Language Modeling task works:\n",
    "\n",
    "1. **Text Masking:** In the MLM task, a random subset of words or tokens in a sentence is selected for masking. These selected tokens are typically chosen with a certain probability, such as 15% of the total tokens in a sentence. The selected tokens are then replaced with a special token, often [MASK].\n",
    "\n",
    "2. **Objective:** The objective of the model during training is to predict the original words that have been masked. This is done by applying the model to the sentence with masked tokens and having it predict the most likely tokens for each [MASK] position.\n",
    "\n",
    "3. **Training Data:** The training data for MLM consists of a large corpus of text, where sentences or documents are randomly selected, and the MLM task is applied. The masked sentences, along with the original sentences, are used to train the model. The model learns to generate contextual embeddings for each word/token in the sentence based on the surrounding context, making it bidirectional in its understanding.\n",
    "\n",
    "4. **Contextual Learning:** MLM encourages the model to capture contextual relationships between words. For example, consider the sentence: \"The cat sat on the [MASK].\" To predict the masked word, the model should take into account both the words before and after the [MASK] token. It learns to recognize that \"cat\" and \"sat\" are more likely to be in the context of each other.\n",
    "\n",
    "5. **Training Objectives:** During MLM training, the model minimizes a loss function that measures the difference between the predicted probabilities of words and the actual masked words. The most common loss function used is the cross-entropy loss.\n",
    "\n",
    "6. **Bi- and Multi-Lingual Learning:** MLM training can be done for single-language models or multilingual models. In the case of multilingual models, the model is trained on text from multiple languages, allowing it to understand and generate text in various languages.\n",
    "\n",
    "The benefits of the MLM task are as follows:\n",
    "\n",
    "- **Bidirectional Context:** Unlike traditional language models that predict words left-to-right or right-to-left, MLM encourages models to consider both directions. This bidirectional understanding is a key feature of models like BERT.\n",
    "\n",
    "- **Contextual Embeddings:** The model learns to generate contextual embeddings for each word/token, making the representations richer and more informative.\n",
    "\n",
    "- **Pretrained Knowledge:** Pretraining with MLM allows models to capture general linguistic knowledge from a vast amount of text, making them effective for a wide range of downstream NLP tasks.\n",
    "\n",
    "MLM is an essential component of many state-of-the-art NLP models, and pretrained models using MLM, like BERT, have achieved remarkable success in various NLP benchmarks and applications. These models are often fine-tuned on specific tasks to leverage the knowledge learned during MLM pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5906a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Explain Next Sentence Prediction (NSP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bac4c2",
   "metadata": {},
   "source": [
    "Next Sentence Prediction (NSP) is another unsupervised pretraining task used in natural language processing (NLP), particularly in transformer-based models like BERT (Bidirectional Encoder Representations from Transformers). The NSP task is designed to help models understand the relationships between sentences in a document by predicting whether one sentence logically follows another.\n",
    "\n",
    "Here's how the Next Sentence Prediction task works:\n",
    "\n",
    "1. **Sentence Pairs:** In the NSP task, pairs of sentences are constructed from a corpus of text data. These pairs consist of two consecutive sentences randomly chosen from the text. One sentence is designated as the \"current\" sentence (sentence A), and the other is designated as the \"next\" sentence (sentence B).\n",
    "\n",
    "2. **Labeling Sentences:** Each sentence pair is labeled with a binary classification label:\n",
    "   - Label 1: If sentence B is the actual next sentence that follows sentence A in the text.\n",
    "   - Label 0: If sentence B is a randomly chosen sentence from the corpus and does not follow sentence A.\n",
    "\n",
    "3. **Objective:** The objective of the model during training is to predict the correct label (1 or 0) for each sentence pair. The model must learn to recognize whether sentence B logically follows sentence A or not.\n",
    "\n",
    "4. **Training Data:** The training data for the NSP task consists of sentence pairs with their corresponding labels. These sentence pairs are used to train the model.\n",
    "\n",
    "5. **Model Learning:** The model is trained to minimize a binary cross-entropy loss, which measures the difference between its predicted probability for the correct label and the actual label.\n",
    "\n",
    "6. **Contextual Understanding:** By training on the NSP task, the model learns to understand contextual relationships between sentences. For example, it learns to recognize that \"The weather is nice today. I'll go for a walk.\" is a valid pair, whereas \"The weather is nice today. Giraffes are tall.\" is not.\n",
    "\n",
    "7. **Applications:** The understanding of sentence relationships gained through the NSP task can be beneficial for various NLP applications. It allows models to capture document-level context and coherence, which is essential for tasks like document summarization, question answering, and textual entailment.\n",
    "\n",
    "The NSP task complements the Masked Language Modeling (MLM) task used in models like BERT. While MLM focuses on understanding word relationships within sentences, NSP emphasizes understanding relationships between sentences in a document. These two pretraining tasks together provide a comprehensive understanding of text, making pretrained models versatile and effective for various downstream NLP tasks.\n",
    "\n",
    "Overall, NSP is an important component of the pretraining process for transformer-based models and contributes to their ability to handle complex language understanding tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What is Matthews evaluation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2d630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a74b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What is Matthews Correlation Coefficient (MCC)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a5cf8",
   "metadata": {},
   "source": [
    "Matthews Correlation Coefficient (MCC) is a metric used for evaluating the performance of classification models, particularly in situations where the class distribution is imbalanced. It takes into account true positives, true negatives, false positives, and false negatives to provide a balanced measure of classification performance. MCC is especially useful when dealing with binary classification tasks.\n",
    "\n",
    "The MCC formula is as follows:\n",
    "\n",
    "\\[MCC = \\frac{(TP \\times TN) - (FP \\times FN)}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\\]\n",
    "\n",
    "Where:\n",
    "- \\(TP\\) (True Positives) represents the number of instances correctly classified as positive.\n",
    "- \\(TN\\) (True Negatives) represents the number of instances correctly classified as negative.\n",
    "- \\(FP\\) (False Positives) represents the number of instances incorrectly classified as positive when they are actually negative.\n",
    "- \\(FN\\) (False Negatives) represents the number of instances incorrectly classified as negative when they are actually positive.\n",
    "\n",
    "The MCC ranges from -1 to 1:\n",
    "- MCC = 1 indicates perfect prediction.\n",
    "- MCC = 0 indicates random prediction (no better than chance).\n",
    "- MCC = -1 indicates perfect inverse prediction.\n",
    "\n",
    "Key points about the Matthews Correlation Coefficient:\n",
    "\n",
    "1. **Balanced Metric:** MCC is considered a balanced metric because it takes into account both positive and negative classifications. This makes it particularly useful in situations where class distribution is imbalanced, and one class significantly outnumbers the other.\n",
    "\n",
    "2. **Sensitivity to Imbalance:** MCC is sensitive to imbalanced datasets and can provide more reliable performance measurements than accuracy, which can be misleading in imbalanced scenarios.\n",
    "\n",
    "3. **Ranges:** The MCC value ranges from -1 (perfect inverse prediction) to 1 (perfect prediction), with 0 indicating random prediction. Values greater than 0 suggest that the model is performing better than random.\n",
    "\n",
    "4. **Applicability:** MCC is commonly used in various fields, including medical research, biology, and machine learning, to assess the performance of binary classification models, such as those used in disease diagnosis, sentiment analysis, and fraud detection.\n",
    "\n",
    "5. **Limitations:** Like other metrics, MCC may not provide a complete picture of model performance, especially in multi-class classification problems. In such cases, it is often necessary to consider additional metrics, such as precision, recall, and F1-score.\n",
    "\n",
    "In summary, the Matthews Correlation Coefficient is a valuable metric for assessing the performance of binary classification models, especially when dealing with imbalanced datasets. It provides a balanced measure of classification quality by taking into account true positives, true negatives, false positives, and false negatives, making it a useful tool in various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2558c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Explain Semantic Role Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3cedde",
   "metadata": {},
   "source": [
    "Semantic Role Labeling (SRL) is a natural language processing (NLP) task that aims to identify and classify the semantic roles of words or phrases in a sentence, with respect to a predicate (typically a verb). It involves determining the relationships between the words in a sentence and their roles in conveying the meaning of an action or event. SRL helps in understanding the underlying structure and meaning of a sentence by specifying who does what to whom.\n",
    "\n",
    "Key components and concepts of Semantic Role Labeling include:\n",
    "\n",
    "1. **Predicates:** Predicates are typically verbs in a sentence, but they can also be other words that convey actions or events. The goal of SRL is to identify these predicates and determine their roles in the sentence.\n",
    "\n",
    "2. **Roles:** Roles represent the semantic functions or relationships between words in a sentence and a predicate. Common semantic roles include:\n",
    "   - Agent: The entity performing the action.\n",
    "   - Patient: The entity affected by the action.\n",
    "   - Instrument: The tool or means used to perform the action.\n",
    "   - Location: The place where the action takes place.\n",
    "   - Time: The time or temporal information related to the action.\n",
    "   - Beneficiary: The entity for whose benefit the action is performed.\n",
    "   - etc.\n",
    "\n",
    "3. **Argument Identification:** In SRL, the first step is to identify the words or phrases in a sentence that serve as arguments of the predicate. These arguments are typically nouns or noun phrases, and they can take on various semantic roles.\n",
    "\n",
    "4. **Role Labeling:** After identifying the arguments, the next step is to assign specific roles to each argument with respect to the predicate. For example, identifying the agent of an action and labeling it as \"Agent\" or identifying the patient and labeling it as \"Patient.\"\n",
    "\n",
    "5. **Syntactic Parsing:** SRL often relies on syntactic parsing to determine the grammatical structure of the sentence. Understanding the sentence's syntax helps in correctly identifying the arguments and their roles.\n",
    "\n",
    "6. **FrameNet and PropBank:** SRL systems are often trained on large annotated corpora, such as FrameNet and PropBank, which provide extensive examples of predicate-argument structures in sentences. These resources serve as references for training and evaluation.\n",
    "\n",
    "Applications of Semantic Role Labeling:\n",
    "- **Question Answering:** SRL can help in extracting relevant information from a sentence to answer questions accurately.\n",
    "- **Information Extraction:** SRL aids in identifying and extracting structured information from unstructured text.\n",
    "- **Machine Translation:** It can assist in generating more contextually accurate translations by understanding the roles of words in sentences.\n",
    "- **Text Summarization:** SRL can help in extracting the most important information and relationships between entities in a text for summarization.\n",
    "- **Sentiment Analysis:** Understanding the semantic roles of entities in sentences can improve sentiment analysis by identifying the agent, target, and action of sentiment expressions.\n",
    "\n",
    "Overall, Semantic Role Labeling plays a crucial role in deepening the understanding of the meaning and structure of natural language sentences, enabling more sophisticated language understanding and processing in various NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbe60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Why Fine-tuning a BERT model takes less time than pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2464dc",
   "metadata": {},
   "source": [
    "Fine-tuning a pretrained BERT (Bidirectional Encoder Representations from Transformers) model typically takes less time than the original pretraining process for several reasons:\n",
    "\n",
    "1. **Transfer Learning:** The most time-consuming part of training large language models like BERT is the initial pretraining phase, where the model is trained on a massive corpus of text data. During this phase, the model learns general language understanding and representations. Fine-tuning, on the other hand, leverages these pretrained representations, so it doesn't require starting from scratch.\n",
    "\n",
    "2. **Data Efficiency:** Fine-tuning typically requires less labeled data compared to the large, diverse, and unlabeled datasets used in pretraining. Since the pretrained model already has a strong foundation in language understanding, it can often achieve good performance with relatively small amounts of task-specific labeled data.\n",
    "\n",
    "3. **Fewer Training Steps:** Pretraining BERT often requires training for hundreds of thousands to millions of steps. In contrast, fine-tuning is performed on top of a pretrained model, and it usually involves training for a much smaller number of steps (e.g., thousands or tens of thousands). This reduces the overall training time.\n",
    "\n",
    "4. **Lower Computational Cost:** Fine-tuning is computationally less expensive than pretraining because it doesn't require the same scale of computational resources, such as distributed training across multiple GPUs or TPUs. Fine-tuning can often be done on a single GPU or even a CPU, making it more accessible.\n",
    "\n",
    "5. **Fixed Architecture:** During fine-tuning, the architecture of the model remains fixed, and only the task-specific layers on top of the pretrained model are trained. This means that the bulk of the model parameters (pretrained layers) remains unchanged, and only a small fraction is updated during fine-tuning. This significantly reduces the number of parameters that need to be learned.\n",
    "\n",
    "6. **Task-Specific Adaptation:** Fine-tuning is focused on adapting the pretrained model to a specific downstream task, such as sentiment analysis, text classification, or named entity recognition. This task-specific adaptation is less complex and requires fewer iterations compared to the diverse, unsupervised learning tasks involved in pretraining.\n",
    "\n",
    "7. **Hyperparameter Tuning:** In fine-tuning, you often have the option to reuse hyperparameters from the pretrained model, such as learning rates and model architecture, which can save time compared to the extensive hyperparameter search often required in pretraining.\n",
    "\n",
    "It's important to note that fine-tuning is a crucial step in applying pretrained language models to specific NLP tasks, as it allows the model to adapt to the task's requirements and achieve state-of-the-art performance with significantly less computational and time resources than would be required for pretraining from scratch. Fine-tuning has made these pretrained models highly practical and widely accessible for a wide range of NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a730ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Recognizing Textual Entailment (RTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21259fc",
   "metadata": {},
   "source": [
    "Recognizing Textual Entailment (RTE) is a natural language processing (NLP) task that involves determining whether a given natural language text (referred to as the \"hypothesis\") logically entails another text (referred to as the \"text\" or \"premise\"). In simpler terms, RTE aims to establish whether the meaning of the hypothesis can be inferred or logically derived from the meaning of the text or premise.\n",
    "\n",
    "The RTE task is typically framed as a binary classification problem, where the goal is to predict one of two labels:\n",
    "\n",
    "1. **Entailment (E):** If the hypothesis can be logically inferred or entails the information presented in the premise.\n",
    "2. **Non-entailment (N):** If the hypothesis does not logically follow from the premise.\n",
    "\n",
    "Key components and challenges of Recognizing Textual Entailment include:\n",
    "\n",
    "1. **Semantic Understanding:** RTE requires a deep understanding of the semantic relationships between words, phrases, and sentences. It goes beyond surface-level syntactic analysis to assess the logical coherence of the content.\n",
    "\n",
    "2. **Lexical and Syntactic Variability:** Recognizing textual entailment must account for variations in word choice, sentence structure, and expression while still identifying the underlying semantic relationships.\n",
    "\n",
    "3. **Inference Levels:** RTE can involve different levels of logical inference, from straightforward entailment (e.g., \"A cat is an animal, and the text mentions 'a cat,' so the hypothesis 'it is an animal' entails the text\") to more complex reasoning (e.g., \"The text mentions 'swimming in the ocean,' and the hypothesis 'it's a water activity' entails the text\").\n",
    "\n",
    "4. **Applications:** RTE has applications in various NLP tasks, such as question answering, information retrieval, and machine translation, where determining the logical relationship between texts is essential for generating accurate and meaningful responses.\n",
    "\n",
    "5. **Datasets:** RTE research often involves the use of benchmark datasets, such as the Recognizing Textual Entailment (RTE) series of challenges, which provide pairs of texts and hypotheses along with human-annotated labels.\n",
    "\n",
    "Example:\n",
    "- Text (Premise): \"The sun rises in the east.\"\n",
    "- Hypothesis: \"The sun sets in the west.\"\n",
    "\n",
    "In this example, the hypothesis does not logically follow from the text, so it would be labeled as \"Non-entailment.\"\n",
    "\n",
    "RTE systems typically use a combination of linguistic analysis, machine learning models, and deep learning architectures, including neural networks, to perform the classification task. These systems aim to capture the semantic relationships between words and phrases in order to make accurate entailment predictions.\n",
    "\n",
    "Overall, Recognizing Textual Entailment is an important task in NLP, as it contributes to the understanding of the logical relationships between text pairs and enables the development of more sophisticated language understanding systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b97f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Explain the decoder stack of GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1d1cd",
   "metadata": {},
   "source": [
    "The decoder stack of GPT (Generative Pre-trained Transformer) models, such as GPT-2 and GPT-3, plays a crucial role in generating coherent and contextually relevant text. GPT models are autoregressive language models designed for tasks like text generation, text completion, and text classification. The decoder stack is responsible for generating text word-by-word based on the context provided by the preceding words.\n",
    "\n",
    "Here are the key components and features of the decoder stack in GPT models:\n",
    "\n",
    "1. **Transformer Decoder Architecture:** GPT models use the transformer architecture, particularly the decoder part of the transformer, which is responsible for autoregressive generation. The decoder consists of multiple identical layers stacked on top of each other.\n",
    "\n",
    "2. **Positional Encodings:** Similar to the encoder stack, the decoder stack also incorporates positional encodings to provide information about the position of each word in the input sequence. These positional encodings are added to the word embeddings to give the model knowledge of word order.\n",
    "\n",
    "3. **Self-Attention Mechanism:** Each layer in the decoder stack contains a self-attention mechanism, which allows the model to attend to different parts of the input sequence, including previously generated words in the autoregressive process. Self-attention helps capture dependencies and context within the text.\n",
    "\n",
    "4. **Multi-Head Attention:** GPT models use multi-head attention, allowing the model to focus on different parts of the input sequence simultaneously. This is particularly useful for capturing various relationships and dependencies in the data.\n",
    "\n",
    "5. **Feedforward Neural Networks:** After self-attention, each layer in the decoder stack includes a feedforward neural network, which processes the attended input to produce intermediate representations.\n",
    "\n",
    "6. **Layer Normalization and Residual Connections:** Layer normalization and residual connections are applied after each sublayer (self-attention and feedforward) in each decoder layer. These techniques help stabilize training and enable the model to learn more effectively.\n",
    "\n",
    "7. **Stacking Decoder Layers:** GPT models typically stack multiple decoder layers (e.g., 12 or 24 layers in GPT-2) to capture increasingly complex patterns and dependencies in the text. The output of one layer serves as the input to the next layer.\n",
    "\n",
    "8. **Vocabulary and Softmax:** At the output of the decoder stack, a softmax layer is used to produce a probability distribution over the entire vocabulary. This distribution is used to sample the next word in the generated sequence.\n",
    "\n",
    "9. **Autoregressive Generation:** During text generation, GPT models generate text one word at a time, with each word conditioned on the preceding words. This autoregressive process allows the model to consider context and generate coherent and contextually relevant text.\n",
    "\n",
    "10. **Beam Search (Optional):** While not a part of the decoder stack itself, GPT models often use beam search during text generation to find more optimal sequences of words by exploring multiple candidates at each step.\n",
    "\n",
    "The decoder stack in GPT models is designed to capture the dependencies, context, and patterns in the text, making them highly effective for various text generation tasks, including language modeling, text completion, and text generation from prompts. The architecture's autoregressive nature allows GPT models to generate fluent and coherent text, making them valuable in a wide range of natural language processing applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
