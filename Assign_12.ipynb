{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae7c884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is prior probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37acdae3",
   "metadata": {},
   "source": [
    "Covariance is a statistical measure that quantifies the degree to which two random variables change together. It indicates whether an increase or decrease in one variable is associated with a similar increase or decrease in another variable. In other words, covariance measures the linear association between two variables.\n",
    "\n",
    "Here are some key points about covariance:\n",
    "\n",
    "1. **Positive Covariance:** If the covariance between two variables is positive, it indicates that when one variable increases, the other tends to increase as well. Similarly, when one decreases, the other tends to decrease. This suggests a positive linear relationship.\n",
    "\n",
    "2. **Negative Covariance:** If the covariance is negative, it means that when one variable increases, the other tends to decrease, and vice versa. This suggests a negative linear relationship.\n",
    "\n",
    "3. **Zero Covariance:** A covariance of zero suggests that there is no linear relationship between the two variables. However, it does not imply that there is no relationship or dependence between the variables; it simply means that any relationship is not linear.\n",
    "\n",
    "The formula for calculating the covariance between two random variables X and Y, based on a sample of data, is as follows:\n",
    "\n",
    "\\[ \\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y}) \\]\n",
    "\n",
    "Where:\n",
    "- \\(\\text{Cov}(X, Y)\\) is the covariance between X and Y.\n",
    "- \\(n\\) is the number of data points in the sample.\n",
    "- \\(X_i\\) and \\(Y_i\\) are individual data points for X and Y, respectively.\n",
    "- \\(\\bar{X}\\) and \\(\\bar{Y}\\) are the sample means of X and Y, respectively.\n",
    "\n",
    "Covariance is a useful measure in statistics and data analysis, as it helps assess the degree of linear association between two variables. However, it is not standardized and can be influenced by the scales of the variables, making it difficult to compare across datasets. To address this, the concept of correlation is often used, which standardizes covariance to produce correlation coefficients with values between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c706895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What is posterior probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8a079",
   "metadata": {},
   "source": [
    "Posterior probability, in the context of probability theory and statistics, refers to the updated probability assigned to an event or hypothesis after taking into account new evidence or information. It represents the revised probability estimate based on both prior beliefs and observed data.\n",
    "\n",
    "Here's an example to illustrate posterior probability:\n",
    "\n",
    "**Example:** Consider a medical diagnostic scenario where a patient is being tested for a specific disease, say Disease X. Before any tests are conducted, the doctor has a prior probability estimate based on general knowledge or prevalence rates, as discussed in the previous answer. Let's assume the prior probability of having Disease X is 0.001 (0.1%) based on population statistics.\n",
    "\n",
    "Now, the patient undergoes a diagnostic test for Disease X, and the test result is positive, indicating the presence of antibodies associated with the disease. This test result serves as new evidence.\n",
    "\n",
    "To calculate the posterior probability of the patient having Disease X, Bayes' theorem is used. Bayes' theorem combines the prior probability with the likelihood of observing the test result given different hypotheses (presence or absence of the disease). The formula for Bayes' theorem is as follows:\n",
    "\n",
    "\\[ P(\\text{Disease X|Positive Test}) = \\frac{P(\\text{Positive Test|Disease X}) \\cdot P(\\text{Disease X})}{P(\\text{Positive Test})} \\]\n",
    "\n",
    "- \\( P(\\text{Disease X|Positive Test}) \\) is the posterior probability of having Disease X given a positive test result.\n",
    "- \\( P(\\text{Positive Test|Disease X}) \\) is the likelihood of obtaining a positive test result if the patient has Disease X.\n",
    "- \\( P(\\text{Disease X}) \\) is the prior probability of having Disease X.\n",
    "- \\( P(\\text{Positive Test}) \\) is the total probability of obtaining a positive test result, considering all possible scenarios.\n",
    "\n",
    "Suppose the test for Disease X has a high true positive rate (accuracy) of 0.95, meaning that it correctly identifies 95% of cases with the disease. Therefore:\n",
    "\n",
    "- \\( P(\\text{Positive Test|Disease X}) = 0.95 \\)\n",
    "\n",
    "Using Bayes' theorem, you can calculate the posterior probability based on the prior probability and the test's accuracy.\n",
    "\n",
    "The resulting posterior probability will reflect the updated belief about the patient's condition after considering the positive test result. It represents the probability of actually having the disease given the new evidence.\n",
    "\n",
    "In this example, the posterior probability will likely be higher than the prior probability, indicating increased confidence in the diagnosis after taking the test result into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6859de",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What is likelihood probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c98bce",
   "metadata": {},
   "source": [
    "Likelihood probability, in the context of probability theory and statistics, represents the probability of observing a particular set of data or evidence given a specific hypothesis or parameter value. It quantifies how well the data supports or fits a particular hypothesis. It's important to note that likelihood is not a probability distribution over hypotheses; instead, it assesses how probable the observed data is under different hypotheses.\n",
    "\n",
    "Here's an example to illustrate likelihood probability:\n",
    "\n",
    "**Example:** Suppose you are flipping a fair six-sided die, and you want to estimate the probability that the die is biased in favor of showing a 6. You decide to conduct an experiment by rolling the die 10 times and recording the results:\n",
    "\n",
    "- Out of the 10 rolls, you observe that the die shows a 6 five times (5 successes) and a non-6 number five times (5 failures).\n",
    "\n",
    "Now, you want to assess whether the die is biased in favor of showing a 6. You can formulate two hypotheses:\n",
    "\n",
    "- Hypothesis H1: The die is unbiased, and each face (1 through 6) has an equal probability of 1/6.\n",
    "- Hypothesis H2: The die is biased in favor of showing a 6, and the probability of rolling a 6 is greater than 1/6.\n",
    "\n",
    "To determine which hypothesis is more supported by the observed data, you can calculate the likelihood for each hypothesis:\n",
    "\n",
    "- For H1 (unbiased die): The likelihood of observing five 6's and five non-6's in 10 rolls, assuming an equal probability for each face, can be calculated using the binomial probability formula. This gives you the likelihood under H1.\n",
    "- For H2 (biased die): The likelihood of observing the same outcome under the assumption of a biased die can be calculated based on the specific probability of rolling a 6 that you want to test.\n",
    "\n",
    "The likelihoods are calculated separately for each hypothesis and compared. The hypothesis with the higher likelihood given the observed data is considered more supported by the evidence.\n",
    "\n",
    "In this example, if the likelihood under H2 (biased die) is substantially higher than the likelihood under H1 (unbiased die), it suggests that the observed data is more consistent with the hypothesis that the die is biased in favor of showing a 6. Likelihood provides a quantitative way to assess the strength of evidence in favor of different hypotheses based on observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce314bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What is Naïve Bayes classifier? Why is it named so?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b7ad0",
   "metadata": {},
   "source": [
    "The Naïve Bayes classifier is a probabilistic machine learning algorithm used for classification tasks, such as text classification, spam detection, sentiment analysis, and more. It's based on the principles of Bayes' theorem and conditional probability.\n",
    "\n",
    "The term \"Naïve\" in Naïve Bayes comes from the assumption that the algorithm makes about the independence of features. Specifically, it assumes that all features (or variables) used for classification are conditionally independent given the class label. In other words, the presence or absence of one feature is not dependent on the presence or absence of any other feature, given the class label. This is a simplified and often unrealistic assumption, hence the term \"Naïve.\"\n",
    "\n",
    "Despite this simplifying assumption, Naïve Bayes classifiers are surprisingly effective in many real-world classification tasks, particularly in natural language processing (NLP) and text classification. They are known for their simplicity, ease of implementation, and computational efficiency.\n",
    "\n",
    "The Naïve Bayes classifier calculates the probability of a given instance belonging to a particular class by combining the prior probability (the probability of the class occurring without any information about the instance) and the likelihood (the probability of the instance's features given the class). Mathematically, it's expressed as:\n",
    "\n",
    "\\[ P(\\text{Class} | \\text{Instance}) = \\frac{P(\\text{Instance} | \\text{Class}) \\cdot P(\\text{Class})}{P(\\text{Instance})} \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(\\text{Class} | \\text{Instance}) \\) is the posterior probability of the instance belonging to the class.\n",
    "- \\( P(\\text{Instance} | \\text{Class}) \\) is the likelihood probability (the probability of the instance's features given the class).\n",
    "- \\( P(\\text{Class}) \\) is the prior probability of the class.\n",
    "- \\( P(\\text{Instance}) \\) is the probability of the instance (used for normalization).\n",
    "\n",
    "Naïve Bayes classifiers come in different variants, including:\n",
    "1. Multinomial Naïve Bayes: Suitable for discrete count data, often used in text classification.\n",
    "2. Gaussian Naïve Bayes: Assumes that continuous features follow a Gaussian distribution.\n",
    "3. Bernoulli Naïve Bayes: Suitable for binary data.\n",
    "\n",
    "Despite the simplifying assumption of independence, Naïve Bayes classifiers can perform surprisingly well when applied to real-world problems, especially when you have limited training data and want a quick and interpretable classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b432e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What is optimal Bayes classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9595a553",
   "metadata": {},
   "source": [
    "The Optimal Bayes Classifier, also known as the Bayes Optimal Classifier or the Bayes Error Rate, is a theoretical concept in machine learning and statistics. It represents the ideal or best possible classifier for a given classification problem. In essence, it's a benchmark against which other classification algorithms are compared.\n",
    "\n",
    "The Optimal Bayes Classifier is \"optimal\" because it minimizes the classification error rate, achieving the lowest possible error rate for a given distribution of data and class labels. It does this by making classification decisions based on the posterior probabilities of the classes, calculated using Bayes' theorem.\n",
    "\n",
    "Here's how the Optimal Bayes Classifier works:\n",
    "\n",
    "1. For each possible class, it computes the posterior probability that a given instance belongs to that class, given the observed features.\n",
    "\n",
    "2. The classifier assigns the instance to the class with the highest posterior probability. In other words, it chooses the class that is most likely given the observed data.\n",
    "\n",
    "Mathematically, the decision rule for the Optimal Bayes Classifier is:\n",
    "\n",
    "\\[ \\hat{y} = \\arg\\max_{c} P(C=c | X) \\]\n",
    "\n",
    "Where:\n",
    "- \\( \\hat{y} \\) is the predicted class label.\n",
    "- \\( c \\) represents each possible class.\n",
    "- \\( P(C=c | X) \\) is the posterior probability of class \\( c \\) given the observed features \\( X \\).\n",
    "\n",
    "The Optimal Bayes Classifier is \"optimal\" in the sense that it minimizes the probability of making a classification error when the true underlying distribution of data is known. However, in practice, the true distribution is rarely known, and we often have to rely on estimates from the training data.\n",
    "\n",
    "In real-world applications, the Optimal Bayes Classifier serves as a theoretical benchmark, indicating the lowest achievable error rate for a given classification problem. Other classification algorithms, such as decision trees, support vector machines, and neural networks, are evaluated based on how close they come to this optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b5590",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Write any two features of Bayesian learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d5790",
   "metadata": {},
   "source": [
    "Bayesian learning methods are characterized by several features that distinguish them in the field of machine learning. Two key features of Bayesian learning methods are:\n",
    "\n",
    "1. **Probabilistic Framework:** Bayesian learning methods are inherently probabilistic. They view all model parameters and predictions as probability distributions rather than fixed values. This probabilistic framework allows Bayesian models to provide not only point estimates but also uncertainty estimates for predictions. Instead of giving a single answer, Bayesian models provide a distribution over possible outcomes, which can be particularly useful in situations where uncertainty is critical, such as medical diagnosis or financial forecasting.\n",
    "\n",
    "2. **Incorporation of Prior Knowledge:** Bayesian learning methods allow the incorporation of prior knowledge or beliefs about the problem into the modeling process. This is achieved through the use of prior probability distributions over model parameters. The prior encodes what is known or assumed about the parameters before observing any data. As new data is observed, the prior is updated to form a posterior probability distribution over parameters, which represents the updated knowledge after considering the data. This process is known as Bayesian inference, and it enables Bayesian models to learn from both data and prior information, making them adaptable to a wide range of scenarios.\n",
    "\n",
    "These features make Bayesian learning methods well-suited for tasks that involve uncertainty modeling, small data sets, and scenarios where prior knowledge is available or relevant. They are used in various machine learning algorithms, including Bayesian networks, Bayesian linear regression, and Bayesian deep learning, to name a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ed2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45b43fca",
   "metadata": {},
   "source": [
    "7. Define the concept of consistent learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756d970",
   "metadata": {},
   "source": [
    "In machine learning, a consistent learner refers to an algorithm or a learning model that, as the amount of training data increases indefinitely, converges to the correct target function or hypothesis. In other words, a consistent learner is one that will eventually learn the true underlying relationship or pattern in the data as more and more training examples become available.\n",
    "\n",
    "Key characteristics of consistent learners include:\n",
    "\n",
    "1. **Convergence:** As the number of training examples grows, a consistent learner converges to the optimal hypothesis or target function. This means that the learner's performance, as measured by its error or misclassification rate, approaches the best possible performance achievable given the data.\n",
    "\n",
    "2. **No Overfitting:** Consistent learners do not overfit the training data. Overfitting occurs when a learning algorithm fits the training data too closely, capturing noise or irrelevant patterns in the data. A consistent learner, by definition, avoids overfitting and generalizes well to unseen data.\n",
    "\n",
    "3. **Statistical Consistency:** Consistency is often defined in a statistical sense. It means that as the number of training examples approaches infinity, the learner's estimate of the true underlying probability distribution of the data becomes increasingly accurate. This ensures that the learner's predictions are asymptotically correct.\n",
    "\n",
    "Consistency is a desirable property for machine learning algorithms because it provides theoretical guarantees that the algorithm will learn the correct concept or model as more data becomes available. However, it's important to note that not all learning algorithms are consistent, and the concept of consistency is closely tied to the choice of learning algorithm, its complexity, and the properties of the data.\n",
    "\n",
    "Consistent learners are particularly important in the context of statistical learning theory and the study of convergence rates. They provide a foundation for understanding the behavior of learning algorithms as the data size increases and form the basis for assessing the performance and reliability of these algorithms in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd6d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Write any two strengths of Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ce363",
   "metadata": {},
   "source": [
    "The Bayes classifier, often referred to as the Naïve Bayes classifier, has several strengths that make it a valuable tool in various machine learning and classification tasks. Two of its key strengths are:\n",
    "\n",
    "1. **Simplicity and Efficiency:** One of the most significant strengths of the Bayes classifier is its simplicity. It is straightforward to understand and implement, even for individuals with limited experience in machine learning. The algorithm's simplicity also translates into computational efficiency, making it computationally lightweight and capable of handling large datasets and real-time applications. Because of its efficiency, Naïve Bayes classifiers are often used in scenarios where quick model deployment and inference are crucial.\n",
    "\n",
    "2. **Effective with High-Dimensional Data:** The Bayes classifier can perform surprisingly well in high-dimensional feature spaces, where the number of features (dimensions) is large. This is especially relevant in text classification and natural language processing tasks, where the feature space can be vast (e.g., the words in a document). Despite its simplicity and the \"naïve\" assumption of feature independence, the Bayes classifier can provide competitive or even superior performance in these high-dimensional settings. It can handle text data, email spam detection, sentiment analysis, and other text-based classification tasks effectively.\n",
    "\n",
    "These strengths make the Bayes classifier a useful choice in situations where simplicity, speed, and performance on high-dimensional data are essential. However, it's important to note that the \"naïve\" assumption of feature independence might not hold in all real-world scenarios, and the classifier's performance can suffer when this assumption is violated. In such cases, more complex models may be necessary, but Naïve Bayes remains a valuable baseline model for benchmarking and initial experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f50038",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Write any two weaknesses of Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03953978",
   "metadata": {},
   "source": [
    "While the Bayes classifier, specifically the Naïve Bayes classifier, has several strengths, it also has some notable weaknesses that can affect its performance in certain scenarios. Two common weaknesses are:\n",
    "\n",
    "1. **Naïve Independence Assumption:** The primary weakness of the Naïve Bayes classifier is its assumption of feature independence. It assumes that all features used for classification are conditionally independent given the class label. In other words, it treats each feature as if it does not depend on or correlate with any other feature, given the class. This assumption is often unrealistic in real-world data, as many features may exhibit dependencies or correlations. When this assumption is violated, the model's accuracy can be compromised, and it may not capture complex relationships among features.\n",
    "\n",
    "2. **Sensitivity to Feature Distribution:** Naïve Bayes classifiers are sensitive to the distribution of features within each class. If the distribution of a particular feature significantly differs between classes, the model may assign disproportionate importance to that feature and perform poorly. For example, if a feature is highly skewed or has outliers in one class but not in others, it can affect the classifier's ability to make accurate predictions.\n",
    "\n",
    "To mitigate these weaknesses, various extensions and adaptations of the Naïve Bayes classifier have been developed. For example:\n",
    "\n",
    "- **Smoothing Techniques:** Laplace smoothing (additive smoothing) and other smoothing methods are used to address issues with zero probabilities in the presence of limited data. These techniques help the classifier handle rare or unseen feature combinations.\n",
    "\n",
    "- **Feature Engineering:** Careful feature selection and engineering can help reduce the impact of the independence assumption. By selecting relevant features and transforming data appropriately, you can improve the model's performance.\n",
    "\n",
    "- **Alternative Bayesian Models:** Bayesian networks and other Bayesian models relax the independence assumption to some extent and can capture more complex dependencies among features. However, they are typically more complex and computationally intensive.\n",
    "\n",
    "It's important to consider these weaknesses when choosing a classification algorithm. While the Naïve Bayes classifier may not be suitable for all scenarios, it serves as a valuable baseline model and can perform well in many practical applications, particularly when feature independence is a reasonable assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e1000",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Explain how Naïve Bayes classifier is used for\n",
    "\n",
    "1. Text classification\n",
    "\n",
    "2. Spam filtering\n",
    "\n",
    "3. Market sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd0209",
   "metadata": {},
   "source": [
    "The Naïve Bayes classifier is a versatile algorithm used in various classification tasks, including text classification, spam filtering, and market sentiment analysis. Here's how it is applied in each of these contexts:\n",
    "\n",
    "1. **Text Classification:**\n",
    "   \n",
    "   Text classification involves categorizing text documents into predefined categories or labels. This is a common application in natural language processing (NLP) for tasks like topic classification, sentiment analysis, and document categorization. The Naïve Bayes classifier is particularly well-suited for text classification due to its simplicity and efficiency.\n",
    "\n",
    "   - **Data Preparation:** In text classification, the training data consists of labeled text documents (e.g., emails, news articles, tweets) and their associated categories or labels (e.g., spam/ham, positive/negative sentiment, topic labels).\n",
    "   \n",
    "   - **Feature Extraction:** Text data needs to be converted into numerical features that the classifier can work with. Common techniques include using the Bag of Words (BoW) model or Term Frequency-Inverse Document Frequency (TF-IDF) representation to convert text into feature vectors.\n",
    "   \n",
    "   - **Training:** The Naïve Bayes classifier estimates the conditional probabilities of features (words or terms) given each class label. It calculates the prior probabilities of each class based on the frequency of class labels in the training data.\n",
    "   \n",
    "   - **Classification:** To classify a new text document, the Naïve Bayes classifier calculates the posterior probabilities of the document belonging to each class given its features. It then assigns the document to the class with the highest posterior probability.\n",
    "   \n",
    "   - **Example:** In sentiment analysis, given a movie review, the Naïve Bayes classifier might determine whether it has a positive or negative sentiment by calculating the probability of positive sentiment and the probability of negative sentiment based on the words in the review.\n",
    "\n",
    "2. **Spam Filtering:**\n",
    "\n",
    "   Spam filtering is a binary classification task where the goal is to classify emails or messages as either spam (unwanted or malicious) or non-spam (legitimate). The Naïve Bayes classifier is a popular choice for this task due to its ability to handle high-dimensional text data efficiently.\n",
    "\n",
    "   - **Data Preparation:** The training data consists of a collection of emails or messages that are labeled as either spam or non-spam. Features are typically derived from the text content of these messages.\n",
    "   \n",
    "   - **Feature Extraction:** Text preprocessing techniques, such as tokenization and stemming, are used to extract meaningful features from the text data. The most common representation is the Bag of Words (BoW) model, where each feature represents the presence or absence of a specific word or term.\n",
    "   \n",
    "   - **Training:** The Naïve Bayes classifier learns the probabilities of individual words or terms appearing in spam and non-spam messages. It calculates the prior probabilities of spam and non-spam based on the training data.\n",
    "   \n",
    "   - **Classification:** When a new email arrives, the Naïve Bayes classifier calculates the probability of it being spam and the probability of it being non-spam based on the words it contains. It assigns the email to the class with the higher probability.\n",
    "   \n",
    "   - **Example:** In a spam filter, the Naïve Bayes classifier might classify an incoming email as spam if it contains words commonly found in spam emails.\n",
    "\n",
    "3. **Market Sentiment Analysis:**\n",
    "\n",
    "   Market sentiment analysis involves assessing and classifying the sentiment expressed in financial news articles, social media posts, or other sources to gauge market sentiment (bullish or bearish). The Naïve Bayes classifier can be used for sentiment analysis by classifying the sentiment of texts related to financial markets.\n",
    "\n",
    "   - **Data Preparation:** The training data consists of financial news articles, social media posts, or other textual sources, along with sentiment labels (e.g., positive, negative, neutral) indicating the sentiment expressed in the text.\n",
    "   \n",
    "   - **Feature Extraction:** Features are extracted from the text, and common representations include TF-IDF or word embeddings to capture the semantic meaning of words.\n",
    "   \n",
    "   - **Training:** The Naïve Bayes classifier learns the conditional probabilities of features (words or terms) given each sentiment class (positive, negative, neutral) and calculates the prior probabilities of each class based on the training data.\n",
    "   \n",
    "   - **Classification:** When analyzing new texts related to financial markets, the Naïve Bayes classifier assigns sentiment labels based on the calculated probabilities. For example, it might classify a news article as having a positive sentiment if the words and phrases in the article indicate optimism about the market.\n",
    "\n",
    "In all of these applications, the Naïve Bayes classifier offers an efficient and interpretable way to perform classification based on probabilistic principles. It's important to note that while the \"naïve\" assumption of feature independence might not hold in practice, Naïve Bayes classifiers often perform surprisingly well in real-world text classification tasks, making them a valuable tool in various domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60b69a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb0e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcb7066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c2442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00000b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b969f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b8ef03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f100f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c78615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1a81b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddfea23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d85c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db40358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
