{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dcc454",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Provide an example of the concepts of Prior, Posterior, and Likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136af3f4",
   "metadata": {},
   "source": [
    "Sure, let's illustrate the concepts of Prior, Posterior, and Likelihood using a simple example involving medical diagnosis:\n",
    "\n",
    "**Scenario**: Consider a medical test for a rare disease, \"Disease X.\" The test results are either \"Positive\" or \"Negative,\" and we want to determine the probability that a patient actually has the disease given their test result.\n",
    "\n",
    "1. **Prior Probability (Prior):**\n",
    "   \n",
    "   The prior probability is our initial belief about the likelihood of a patient having the disease before we have any information from the test. Let's say that Disease X is indeed rare, affecting only 1% of the population. Therefore, our prior probability of a randomly selected person having the disease is:\n",
    "\n",
    "   **Prior Probability (P(Disease X)) = 0.01 (1%)**\n",
    "\n",
    "   This represents our belief based on background knowledge.\n",
    "\n",
    "2. **Likelihood (Likelihood):**\n",
    "\n",
    "   The likelihood is the probability of observing a particular test result (e.g., \"Positive\" or \"Negative\") given the true disease status. In other words, it quantifies how good our test is at correctly identifying the disease.\n",
    "\n",
    "   - If a person has the disease (Disease X), the likelihood of the test being \"Positive\" might be quite high because we expect the test to correctly detect the disease in most cases. Let's assume it's 95%.\n",
    "   \n",
    "   **Likelihood (P(Positive | Disease X)) = 0.95 (95%)**\n",
    "\n",
    "   - If a person does not have the disease, the likelihood of the test being \"Positive\" might still be non-zero because there can be false positives. Let's assume it's 3% (false positive rate).\n",
    "\n",
    "   **Likelihood (P(Positive | No Disease X)) = 0.03 (3%)**\n",
    "\n",
    "   Similarly, we can calculate the likelihood of the test being \"Negative\" for both cases.\n",
    "\n",
    "3. **Posterior Probability (Posterior):**\n",
    "\n",
    "   The posterior probability is what we want to calculate—the probability that a patient actually has the disease (Disease X) given their test result (\"Positive\"). This is the updated probability after considering the test result.\n",
    "\n",
    "   We can calculate the posterior probability using Bayes' theorem:\n",
    "\n",
    "   **P(Disease X | Positive) = [P(Positive | Disease X) * P(Disease X)] / [P(Positive | Disease X) * P(Disease X) + P(Positive | No Disease X) * P(No Disease X)]**\n",
    "\n",
    "   Plugging in our values:\n",
    "\n",
    "   **P(Disease X | Positive) = [0.95 * 0.01] / [0.95 * 0.01 + 0.03 * 0.99] ≈ 0.24 (24%)**\n",
    "\n",
    "   So, given a positive test result, there is a 24% chance that the patient actually has Disease X. This is the updated probability based on the test result (Posterior Probability).\n",
    "\n",
    "This example demonstrates how we start with a prior belief about the likelihood of having the disease, combine it with the test's performance characteristics (likelihood), and calculate the updated probability (posterior) after observing the test result. Bayes' theorem is a powerful tool for updating beliefs in light of new evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a5d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What role does Bayes&#39; theorem play in the concept learning principle?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4279db23",
   "metadata": {},
   "source": [
    "Bayes' theorem plays a fundamental role in the concept learning principle, particularly in the context of probabilistic concept learning and statistical inference. The concept learning principle is a foundational concept in machine learning and cognitive science that involves learning and generalizing concepts or categories from observed data. Bayes' theorem is used to update and refine our beliefs about these concepts as new evidence or data becomes available. Here's how Bayes' theorem is relevant to the concept learning principle:\n",
    "\n",
    "1. **Probabilistic Modeling:** Concept learning often involves dealing with uncertainty and making decisions based on incomplete or noisy data. Bayes' theorem provides a probabilistic framework for modeling and reasoning about this uncertainty. It allows us to calculate the probability of a concept or hypothesis being true given the observed evidence.\n",
    "\n",
    "2. **Prior Beliefs:** Bayes' theorem incorporates prior beliefs or prior probabilities (P(hypothesis)) about the likelihood of different concepts or hypotheses being true before observing any data. These prior beliefs represent our initial assumptions or expectations about the concepts we are trying to learn. They serve as a starting point for the learning process.\n",
    "\n",
    "3. **Likelihood:** The likelihood term in Bayes' theorem (P(data | hypothesis)) represents how well the observed data fits with each hypothesis or concept. It quantifies the probability of observing the data if a particular hypothesis were true. In concept learning, this likelihood captures how well a concept or category describes the observed examples.\n",
    "\n",
    "4. **Posterior Probability:** The key outcome of applying Bayes' theorem in concept learning is the posterior probability (P(hypothesis | data)), which represents the updated probability of a concept or hypothesis being true after considering the observed evidence. This posterior probability reflects our revised beliefs about the concepts based on the available data.\n",
    "\n",
    "5. **Learning and Generalization:** Bayes' theorem enables the learning and generalization of concepts. As new data is encountered, the posterior probabilities are updated, allowing us to refine our understanding of the concepts. Concepts with higher posterior probabilities are more likely to be correct, and this information guides decision-making and generalization to new, unseen data.\n",
    "\n",
    "6. **Decision-Making:** In some cases, concept learning involves making decisions or classifications. Bayes' theorem can be used to calculate the probabilities of different categories or concepts given observed data, aiding in decision-making and classification tasks.\n",
    "\n",
    "7. **Adaptation to New Evidence:** Bayes' theorem allows for continuous adaptation to new evidence. As more data is observed, the posterior probabilities are updated iteratively, allowing the learner to adapt to changing conditions and refine its understanding of the concepts.\n",
    "\n",
    "In summary, Bayes' theorem is a foundational tool for probabilistic concept learning, enabling the integration of prior beliefs with observed evidence to update and refine our understanding of concepts or categories. It provides a principled framework for handling uncertainty and making decisions in concept learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad468ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Offer an example of how the Nave Bayes classifier is used in real life."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7a0ab",
   "metadata": {},
   "source": [
    "Certainly! One common and practical application of the Naïve Bayes classifier in real life is in email spam filtering. The Naïve Bayes classifier is well-suited for this task because it can effectively distinguish between spam (unwanted or malicious) and legitimate (non-spam) emails based on their content and characteristics.\n",
    "\n",
    "Here's how the Naïve Bayes classifier is used in email spam filtering:\n",
    "\n",
    "**Problem:** The goal is to automatically classify incoming emails as either \"Spam\" or \"Not Spam\" (often referred to as \"Ham\").\n",
    "\n",
    "**Steps in Email Spam Filtering Using Naïve Bayes:**\n",
    "\n",
    "1. **Training Data Collection:**\n",
    "   \n",
    "   - A large dataset of labeled emails is collected for training purposes. These emails are manually categorized as either \"Spam\" or \"Not Spam.\"\n",
    "\n",
    "2. **Text Preprocessing:**\n",
    "   \n",
    "   - The text content of each email is preprocessed. This includes tasks like tokenization (splitting text into words or tokens), removing stop words (common words like \"the\" and \"and\"), and stemming (reducing words to their root forms).\n",
    "\n",
    "3. **Feature Extraction:**\n",
    "   \n",
    "   - Features are extracted from the text data to represent each email. The most common representation is the Bag of Words (BoW) model, where each feature represents the presence or absence of a specific word or term in the email. Other representations, like Term Frequency-Inverse Document Frequency (TF-IDF), can also be used.\n",
    "\n",
    "4. **Training the Naïve Bayes Classifier:**\n",
    "   \n",
    "   - The Naïve Bayes classifier is trained using the labeled dataset. It estimates two sets of probabilities:\n",
    "   \n",
    "     - **Prior Probabilities:** The probability of an email being \"Spam\" or \"Not Spam\" based on the frequency of these labels in the training data.\n",
    "     \n",
    "     - **Likelihood Probabilities:** The probability of observing each word or term in the email given its category (either \"Spam\" or \"Not Spam\").\n",
    "\n",
    "5. **Classification of Incoming Emails:**\n",
    "   \n",
    "   - When a new email arrives, it goes through the same text preprocessing and feature extraction steps as the training data.\n",
    "   \n",
    "   - The Naïve Bayes classifier calculates the posterior probabilities of the email being \"Spam\" and \"Not Spam\" based on the observed words and the probabilities estimated during training.\n",
    "   \n",
    "   - The email is classified as \"Spam\" or \"Not Spam\" based on which category has the higher posterior probability.\n",
    "\n",
    "6. **Filtering Spam:**\n",
    "   \n",
    "   - Emails classified as \"Spam\" are filtered out and sent to a designated spam folder or are flagged as potential spam in the user's inbox.\n",
    "   \n",
    "   - Emails classified as \"Not Spam\" are delivered to the inbox.\n",
    "\n",
    "**Example:**\n",
    "   \n",
    "   - If an incoming email contains words often found in spam emails (e.g., \"free,\" \"discount,\" \"guaranteed\"), the Naïve Bayes classifier may assign a higher probability to the \"Spam\" category, leading to its classification as spam.\n",
    "\n",
    "Email spam filtering using Naïve Bayes is widely used by email service providers and email client software to automatically sort and prioritize emails for users, ensuring that spam emails are kept out of the inbox and that legitimate emails are delivered successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f437e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Can the Nave Bayes classifier be used on continuous numeric data? If so, how can you go about\n",
    "doing it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de596451",
   "metadata": {},
   "source": [
    "The Naïve Bayes classifier is primarily designed for categorical and discrete data, such as text data or data with categorical attributes. It's commonly used for tasks like text classification, spam filtering, and sentiment analysis, where the features are typically binary (e.g., word presence or absence) or represent discrete categories.\n",
    "\n",
    "While the traditional Naïve Bayes classifier is not well-suited for continuous numeric data, there are adaptations and variations of the Naïve Bayes algorithm that can handle continuous data. Here are a few approaches to apply Naïve Bayes to continuous numeric data:\n",
    "\n",
    "1. **Gaussian Naïve Bayes:**\n",
    "   \n",
    "   - The Gaussian Naïve Bayes classifier is an extension of the Naïve Bayes algorithm that assumes that continuous features follow a Gaussian (normal) distribution.\n",
    "   \n",
    "   - In this approach, you estimate the mean and standard deviation of each feature for each class (category). When classifying new data, you calculate the likelihood of observing a specific numeric value given the estimated Gaussian parameters.\n",
    "\n",
    "2. **Kernel Density Estimation (KDE):**\n",
    "   \n",
    "   - Instead of making specific assumptions about the distribution of continuous data, you can use Kernel Density Estimation to estimate the probability density functions (PDFs) for each class.\n",
    "   \n",
    "   - KDE is a non-parametric method that can handle various data distributions.\n",
    "\n",
    "3. **Discretization:**\n",
    "   \n",
    "   - You can convert continuous numeric data into discrete bins or categories. This process is called discretization.\n",
    "   \n",
    "   - Once discretized, you can apply the traditional Naïve Bayes classifier as if you were working with categorical data. This approach loses some information in the process, so the choice of bin sizes and boundaries is important.\n",
    "\n",
    "4. **Binning:**\n",
    "   \n",
    "   - Similar to discretization, you can divide the range of continuous values into bins or intervals.\n",
    "   \n",
    "   - You then treat each bin as a separate categorical value and apply the Naïve Bayes classifier accordingly.\n",
    "\n",
    "It's important to note that when working with continuous data, especially when applying Gaussian Naïve Bayes, you need to consider the assumption of independence between features. The \"Naïve\" part of Naïve Bayes assumes that features are conditionally independent within each class. This assumption may not hold in some cases of continuous data.\n",
    "\n",
    "In practice, the choice of which approach to use depends on the specific problem, the distribution of the data, and the performance requirements. Gaussian Naïve Bayes is a common choice for continuous data when the Gaussian assumption is reasonable, but other approaches like KDE or discretization may be more suitable in other cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f7961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What are Bayesian Belief Networks, and how do they work? What are their applications? Are they\n",
    "capable of resolving a wide range of issues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206c8c35",
   "metadata": {},
   "source": [
    "Bayesian Belief Networks (BBNs), also known as Bayesian Networks or Probabilistic Graphical Models, are a type of probabilistic graphical model used for representing and reasoning about uncertainty and probabilistic dependencies among a set of random variables. BBNs are based on the principles of Bayesian probability and graphical modeling. Here's how they work and their applications:\n",
    "\n",
    "**How BBNs Work:**\n",
    "\n",
    "1. **Graphical Representation:** BBNs are represented as directed acyclic graphs (DAGs), where nodes represent random variables or events, and edges represent probabilistic dependencies or causal relationships between them. Each node contains a conditional probability distribution that quantifies the probability of the node given its parent nodes.\n",
    "\n",
    "2. **Conditional Independence:** BBNs exploit the concept of conditional independence. Nodes in the graph are conditionally independent of their non-descendants (nodes that are not reachable by following edges) given their parents. This property simplifies probabilistic inference.\n",
    "\n",
    "3. **Probabilistic Inference:** BBNs are used for probabilistic inference, which means estimating probabilities of events or variables based on available evidence or observed data. They can answer questions like \"What is the probability of event A given evidence B?\"\n",
    "\n",
    "4. **Updating Probabilities:** BBNs can be updated dynamically as new evidence becomes available. When new information is obtained, the probabilities associated with nodes in the graph can be revised to reflect the updated beliefs.\n",
    "\n",
    "**Applications of Bayesian Belief Networks:**\n",
    "\n",
    "BBNs have a wide range of applications in various fields due to their ability to model and reason about uncertain and complex systems. Some common applications include:\n",
    "\n",
    "1. **Medical Diagnosis:** BBNs are used in medical diagnosis to assist physicians in making informed decisions. They can integrate patient symptoms, test results, and medical knowledge to estimate the probability of various diseases.\n",
    "\n",
    "2. **Risk Assessment:** BBNs are used for risk assessment in fields such as finance and insurance. They can model and evaluate risks associated with investments, insurance policies, and financial portfolios.\n",
    "\n",
    "3. **Natural Language Processing:** In NLP, BBNs can be used for tasks like language modeling, speech recognition, and text classification. They can capture dependencies between words and linguistic features.\n",
    "\n",
    "4. **Environmental Modeling:** BBNs are employed in environmental modeling to assess the impact of environmental factors on ecosystems, climate change, and pollution control.\n",
    "\n",
    "5. **Fault Diagnosis:** BBNs are used in engineering and manufacturing for fault diagnosis in complex systems like aircraft, machinery, and electronics. They can identify the causes of failures.\n",
    "\n",
    "6. **Fraud Detection:** BBNs are used in fraud detection systems to identify unusual or suspicious patterns of behavior in financial transactions or online activities.\n",
    "\n",
    "7. **Recommendation Systems:** BBNs can be used in recommendation systems to model user preferences and make personalized recommendations for products, movies, or content.\n",
    "\n",
    "**Limitations and Capabilities:**\n",
    "\n",
    "While BBNs are powerful tools for modeling uncertainty and probabilistic dependencies, they do have limitations:\n",
    "\n",
    "- **Scalability:** BBNs can become computationally expensive for large and complex systems due to the need for probabilistic inference.\n",
    "\n",
    "- **Modeling Assumptions:** BBNs rely on modeling assumptions, such as conditional independence, which may not always hold in real-world scenarios.\n",
    "\n",
    "- **Data Requirements:** BBNs require data to estimate conditional probabilities, and the accuracy of the model depends on the quality and quantity of available data.\n",
    "\n",
    "- **Complexity of Learning:** Learning the structure and parameters of a BBN from data can be challenging.\n",
    "\n",
    "Overall, BBNs are capable of resolving a wide range of issues involving uncertainty and probabilistic reasoning. Their effectiveness depends on the specific problem domain, the quality of the model, and the availability of data for learning and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbac187",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Passengers are checked in an airport screening system to see if there is an intruder. Let I be the\n",
    "random variable that indicates whether someone is an intruder I = 1) or not I = 0), and A be the\n",
    "variable that indicates alarm I = 0). If an intruder is detected with probability P(A = 1|I = 1) = 0.98\n",
    "and a non-intruder is detected with probability P(A = 1|I = 0) = 0.001, an alarm will be triggered,\n",
    "implying the error factor. The likelihood of an intruder in the passenger population is P(I = 1) =\n",
    "0.00001. What are the chances that an alarm would be triggered when an individual is actually an\n",
    "intruder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1adc1",
   "metadata": {},
   "source": [
    "To find the probability that an alarm is triggered when an individual is actually an intruder (i.e., P(I = 1|A = 1)), you can use Bayes' theorem:\n",
    "\n",
    "\\[P(I = 1|A = 1) = \\frac{P(A = 1|I = 1) \\cdot P(I = 1)}{P(A = 1)}\\]\n",
    "\n",
    "We have the following information:\n",
    "\n",
    "- \\(P(A = 1|I = 1) = 0.98\\): Probability that an alarm is triggered when there is an intruder.\n",
    "- \\(P(I = 1) = 0.00001\\): Probability that an individual is an intruder.\n",
    "- \\(P(A = 1|I = 0) = 0.001\\): Probability that an alarm is triggered when there is no intruder.\n",
    "\n",
    "To find \\(P(A = 1)\\), you can use the law of total probability:\n",
    "\n",
    "\\[P(A = 1) = P(A = 1|I = 1) \\cdot P(I = 1) + P(A = 1|I = 0) \\cdot P(I = 0)\\]\n",
    "\n",
    "Since \\(P(I = 0) = 1 - P(I = 1)\\) (complement probability), you can substitute this in:\n",
    "\n",
    "\\[P(A = 1) = P(A = 1|I = 1) \\cdot P(I = 1) + P(A = 1|I = 0) \\cdot (1 - P(I = 1))\\]\n",
    "\n",
    "Now, plug in the values:\n",
    "\n",
    "\\[P(A = 1) = 0.98 \\cdot 0.00001 + 0.001 \\cdot (1 - 0.00001)\\]\n",
    "\n",
    "Calculate this expression to find the value of \\(P(A = 1)\\).\n",
    "\n",
    "Once you have \\(P(A = 1)\\), you can use Bayes' theorem to calculate \\(P(I = 1|A = 1)\\):\n",
    "\n",
    "\\[P(I = 1|A = 1) = \\frac{0.98 \\cdot 0.00001}{P(A = 1)}\\]\n",
    "\n",
    "Calculate \\(P(I = 1|A = 1)\\) using the values you've obtained. This will give you the probability that an alarm is triggered when an individual is actually an intruder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0639765",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. An antibiotic resistance test (random variable T) has 1% false positives (i.e., 1% of those who are\n",
    "not immune to an antibiotic display a positive result in the test) and 5% false negatives (i.e., 1% of\n",
    "those who are not resistant to an antibiotic show a positive result in the test) (i.e. 5 percent of those\n",
    "actually resistant to an antibiotic test negative). Assume that 2% of those who were screened were\n",
    "antibiotic-resistant. Calculate the likelihood that a person who tests positive is actually immune\n",
    "(random variable D)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f94d3b",
   "metadata": {},
   "source": [
    "To calculate the likelihood that a person who tests positive is actually immune (resistant to an antibiotic), we can use Bayes' theorem. Let's define the following random variables:\n",
    "\n",
    "- \\(D\\): Person is immune (resistant) to an antibiotic.\n",
    "- \\(T\\): Person tests positive in the antibiotic resistance test.\n",
    "\n",
    "We want to find \\(P(D = 1|T = 1)\\), which is the probability that a person is immune given that they tested positive.\n",
    "\n",
    "We have the following information:\n",
    "\n",
    "- \\(P(T = 1|D = 0) = 0.01\\): Probability of a false positive (testing positive when not immune).\n",
    "- \\(P(T = 0|D = 1) = 0.05\\): Probability of a false negative (testing negative when immune).\n",
    "- \\(P(D = 1) = 0.02\\): Probability that a person is actually immune (resistant).\n",
    "\n",
    "We can use Bayes' theorem to calculate \\(P(D = 1|T = 1)\\):\n",
    "\n",
    "\\[P(D = 1|T = 1) = \\frac{P(T = 1|D = 1) \\cdot P(D = 1)}{P(T = 1)}\\]\n",
    "\n",
    "Now, we need to calculate \\(P(T = 1)\\), which is the total probability of testing positive:\n",
    "\n",
    "\\[P(T = 1) = P(T = 1|D = 0) \\cdot P(D = 0) + P(T = 1|D = 1) \\cdot P(D = 1)\\]\n",
    "\n",
    "To calculate \\(P(D = 0)\\), we can use the complement probability:\n",
    "\n",
    "\\[P(D = 0) = 1 - P(D = 1)\\]\n",
    "\n",
    "Now, we can substitute these values into the equation for \\(P(T = 1)\\) and then use Bayes' theorem to find \\(P(D = 1|T = 1)\\):\n",
    "\n",
    "\\[P(T = 1) = P(T = 1|D = 0) \\cdot (1 - P(D = 1)) + P(T = 1|D = 1) \\cdot P(D = 1)\\]\n",
    "\n",
    "Finally, calculate \\(P(D = 1|T = 1)\\) using Bayes' theorem with the updated values. This will give you the likelihood that a person who tests positive is actually immune (resistant) to the antibiotic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. In order to prepare for the test, a student knows that there will be one question in the exam that\n",
    "is either form A, B, or C. The chances of getting an A, B, or C on the exam are 30 percent, 20%, and\n",
    "50 percent, respectively. During the planning, the student solved 9 of 10 type A problems, 2 of 10\n",
    "type B problems, and 6 of 10 type C problems.\n",
    "\n",
    "1. What is the likelihood that the student can solve the exam problem?\n",
    "\n",
    "2. Given the student&#39;s solution, what is the likelihood that the problem was of form A?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf753029",
   "metadata": {},
   "source": [
    "To calculate the likelihood that the student can solve the exam problem and the likelihood that the problem was of form A given the student's solution, we can use Bayes' theorem.\n",
    "\n",
    "Let's define the following events:\n",
    "\n",
    "- A: The problem on the exam is of form A.\n",
    "- B: The problem on the exam is of form B.\n",
    "- C: The problem on the exam is of form C.\n",
    "- S: The student can solve the exam problem.\n",
    "\n",
    "We are given:\n",
    "\n",
    "- \\(P(A) = 0.30\\): Probability that the exam problem is of form A.\n",
    "- \\(P(B) = 0.20\\): Probability that the exam problem is of form B.\n",
    "- \\(P(C) = 0.50\\): Probability that the exam problem is of form C.\n",
    "\n",
    "We are also given the following information about the student's preparation:\n",
    "\n",
    "- The student solved 9 out of 10 type A problems.\n",
    "- The student solved 2 out of 10 type B problems.\n",
    "- The student solved 6 out of 10 type C problems.\n",
    "\n",
    "Let's calculate the likelihood that the student can solve the exam problem (P(S)):\n",
    "\n",
    "\\[P(S) = P(S|A) \\cdot P(A) + P(S|B) \\cdot P(B) + P(S|C) \\cdot P(C)\\]\n",
    "\n",
    "Now, we need to calculate the conditional probabilities:\n",
    "\n",
    "- \\(P(S|A)\\) is the probability that the student can solve the exam problem given it's of form A. Since the student solved 9 out of 10 type A problems, \\(P(S|A) = \\frac{9}{10}\\).\n",
    "- \\(P(S|B)\\) is the probability that the student can solve the exam problem given it's of form B. Since the student solved 2 out of 10 type B problems, \\(P(S|B) = \\frac{2}{10}\\).\n",
    "- \\(P(S|C)\\) is the probability that the student can solve the exam problem given it's of form C. Since the student solved 6 out of 10 type C problems, \\(P(S|C) = \\frac{6}{10}\\).\n",
    "\n",
    "Now, calculate \\(P(S)\\) using the probabilities and conditional probabilities above.\n",
    "\n",
    "To calculate the likelihood that the problem was of form A given the student's solution (P(A|S)), we can use Bayes' theorem:\n",
    "\n",
    "\\[P(A|S) = \\frac{P(S|A) \\cdot P(A)}{P(S)}\\]\n",
    "\n",
    "Substitute the values you have calculated to find \\(P(A|S)\\).\n",
    "\n",
    "This will give you the answers to both questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. A bank installs a CCTV system to track and photograph incoming customers. Despite the constant\n",
    "influx of customers, we divide the timeline into 5 minute bins. There may be a customer coming into\n",
    "the bank with a 5% chance in each 5-minute time period, or there may be no customer (again, for\n",
    "simplicity, we assume that either there is 1 customer or none, not the case of multiple customers). If\n",
    "\n",
    "there is a client, the CCTV will detect them with a 99 percent probability. If there is no customer, the\n",
    "camera can take a false photograph with a 10% chance of detecting movement from other objects.\n",
    "\n",
    "1. How many customers come into the bank on a daily basis (10 hours)?\n",
    "\n",
    "2. On a daily basis, how many fake photographs (photographs taken when there is no\n",
    "customer) and how many missed photographs (photographs taken when there is a customer) are\n",
    "there?\n",
    "\n",
    "3. Explain likelihood that there is a customer if there is a photograph?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c08eaf",
   "metadata": {},
   "source": [
    "Let's break down the problem into three parts:\n",
    "\n",
    "**1. How many customers come into the bank on a daily basis (10 hours)?**\n",
    "\n",
    "First, let's calculate the number of 5-minute intervals in 10 hours. There are \\(10 \\times 60 / 5 = 120\\) such intervals in 10 hours.\n",
    "\n",
    "Now, for each 5-minute interval, there's a 5% chance of a customer coming in. To find the expected number of customers over 10 hours, we can multiply the number of intervals by the probability of a customer coming in:\n",
    "\n",
    "Expected number of customers = \\(120 \\times 0.05 = 6\\) customers.\n",
    "\n",
    "So, on average, 6 customers come into the bank in 10 hours.\n",
    "\n",
    "**2. On a daily basis, how many fake photographs and how many missed photographs are there?**\n",
    "\n",
    "For each 5-minute interval:\n",
    "\n",
    "- There's a 5% chance of a customer coming in (true positive).\n",
    "- There's a 95% chance of no customer coming in (true negative).\n",
    "- There's a 10% chance of a false photograph when there's no customer (false positive).\n",
    "- There's a 1% chance of a missed photograph when there's a customer (false negative).\n",
    "\n",
    "Let's calculate the expected numbers:\n",
    "\n",
    "a. Expected number of true positives: \\(120 \\times 0.05 \\times 0.99 = 5.94\\)\n",
    "\n",
    "b. Expected number of true negatives: \\(120 \\times 0.95 = 114\\)\n",
    "\n",
    "c. Expected number of false positives: \\(120 \\times 0.95 \\times 0.10 = 11.4\\)\n",
    "\n",
    "d. Expected number of false negatives: \\(120 \\times 0.05 \\times 0.01 = 0.06\\)\n",
    "\n",
    "So, on average, there are approximately 5.94 missed photographs (false negatives) and 11.4 fake photographs (false positives) on a daily basis.\n",
    "\n",
    "**3. Explain the likelihood that there is a customer if there is a photograph?**\n",
    "\n",
    "To calculate the likelihood that there is a customer if there is a photograph (the probability of a customer given a photograph), we can use Bayes' theorem:\n",
    "\n",
    "Let \\(C\\) be the event that there is a customer, and \\(P\\) be the event that there is a photograph.\n",
    "\n",
    "We want to find \\(P(C|P)\\), which is the probability that there is a customer given that there is a photograph.\n",
    "\n",
    "Using Bayes' theorem:\n",
    "\n",
    "\\[P(C|P) = \\frac{P(P|C) \\cdot P(C)}{P(P)}\\]\n",
    "\n",
    "We already have some of the probabilities:\n",
    "\n",
    "- \\(P(P|C) = 0.01\\) (probability of a photograph when there is a customer).\n",
    "- \\(P(C) = 0.05\\) (probability of a customer).\n",
    "\n",
    "To find \\(P(P)\\), we can use the law of total probability:\n",
    "\n",
    "\\[P(P) = P(P|C) \\cdot P(C) + P(P|¬C) \\cdot P(¬C)\\]\n",
    "\n",
    "Where \\(P(¬C)\\) is the probability of no customer, which is \\(1 - P(C)\\), and \\(P(P|¬C)\\) is the probability of a photograph when there is no customer, which is 0.10.\n",
    "\n",
    "Now, you can calculate \\(P(C|P)\\) using the values you've obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64956092",
   "metadata": {},
   "outputs": [],
   "source": [
    " 10Create the conditional probability table associated with the node Won Toss in the Bayesian Belief\n",
    "net.work to represent the conditional independence assumptions of the Nave Bayes classifier for the\n",
    "match winning prediction problem in Section 6.4.4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f6143b",
   "metadata": {},
   "source": [
    "To create the conditional probability table (CPT) associated with the node \"Won Toss\" in a Bayesian Belief Network (BBN) representing the conditional independence assumptions of a Naive Bayes classifier for match-winning prediction, we need to specify the conditional probabilities for the \"Won Toss\" node given the class variable and other attributes. \n",
    "\n",
    "Let's assume that in this binary classification problem, we have a class variable representing whether a team wins a match (e.g., \"Win\" or \"Lose\"), and we have several attribute nodes (e.g., \"Pitch Type,\" \"Weather,\" \"Team Strength,\" etc.) that influence the outcome of the match.\n",
    "\n",
    "The CPT for \"Won Toss\" would look like this:\n",
    "\n",
    "- Node: Won Toss\n",
    "- Parents: None (Assuming \"Won Toss\" is conditionally independent of other attributes given the class variable)\n",
    "\n",
    "Here's a simplified example of what the CPT might look like:\n",
    "\n",
    "| Class  | P(Won Toss = Yes | Class = Win) | P(Won Toss = No | Class = Win) | P(Won Toss = Yes | Class = Lose) | P(Won Toss = No | Class = Lose) |\n",
    "|--------|------------------|-----------------|--------------------|-------------------|\n",
    "| Win    | 0.8              | 0.2             | 0.6                | 0.4               |\n",
    "| Lose   | 0.3              | 0.7             | 0.5                | 0.5               |\n",
    "\n",
    "In this example, we have assumed that the probability of winning the toss (\"Won Toss = Yes\") is higher when the team eventually wins the match (\"Class = Win\") compared to when it loses (\"Class = Lose\"). Similarly, the probability of not winning the toss (\"Won Toss = No\") is higher when the team loses the match compared to when it wins.\n",
    "\n",
    "Please note that the actual probabilities would be determined based on the dataset and domain knowledge. The values in the CPT should be estimated from training data using techniques like Maximum Likelihood Estimation (MLE) or Laplace smoothing if needed.\n",
    "\n",
    "This CPT represents the conditional independence assumptions of the Naive Bayes classifier for match-winning prediction, where \"Won Toss\" is conditionally independent of other attributes given the class variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
